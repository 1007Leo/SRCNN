{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Div2kDataset, ImageDataset\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, relu=False, prelu=False):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU(inplace=True) if relu else None\n",
    "        self.prelu = nn.PReLU(out_channels) if prelu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        elif self.prelu is not None:\n",
    "            x\n",
    "        return x\n",
    "\n",
    "class Upsampler(nn.Sequential): # pixel shuffle block\n",
    "    def __init__(self, scale, n_feats):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0: # if scale == 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2d(n_feats, 4*n_feats, 3, padding=1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2d(n_feats, 9*n_feats, 3, padding=1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        super(Upsampler, self).__init__(*m)\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, init_value=1e-3):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * self.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight CNN backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, in_chanels, out_channels, kernel_size = 3):\n",
    "        super(ResidualUnit,self).__init__()\n",
    "        self.reduction = nn.Conv2d(in_chanels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.expansion = nn.Conv2d(out_channels, in_chanels, kernel_size, padding=kernel_size//2)\n",
    "        self.relu = nn.PReLU(out_channels)\n",
    "        self.weight1 = Scale(1)\n",
    "        self.weight2 = Scale(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight1(x) + self.weight2(self.expansion(self.relu(self.reduction(x))))\n",
    "\n",
    "class ARFB(nn.Module):\n",
    "    def __init__(self, n_feats):\n",
    "        super(ARFB, self).__init__()\n",
    "        self.ru1 = ResidualUnit(n_feats, n_feats // 2, 3)\n",
    "        self.ru2 = ResidualUnit(n_feats, n_feats // 2, 3)\n",
    "        self.conv1 = Conv(2*n_feats, n_feats, 1, 1, 0, prelu=True)\n",
    "        self.conv3 = Conv(n_feats, n_feats, 3, 1, 1, prelu=True)\n",
    "        self.attention = CA(n_feats)\n",
    "        self.weight1 = Scale(1)\n",
    "        self.weight2 = Scale(1)\n",
    "        self.weight3 = Scale(1)\n",
    "        self.weight4 = Scale(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.ru1(x)\n",
    "        x2 = self.ru2(x1)\n",
    "        x3 = self.conv3(self.attention(self.conv1(torch.cat([self.weight1(x2), self.weight2(x1)], 1))))\n",
    "        return self.weight3(x)+self.weight4(x3)\n",
    "    \n",
    "class CA(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CA, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_du = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // reduction, 1, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // reduction, channel, 1, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "    \n",
    "class HPB(nn.Module):\n",
    "    def __init__(self, n_feats):\n",
    "        super(HPB, self).__init__()\n",
    "        self.encoder = ARFB(n_feats)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
    "        self.decoder_low = ARFB(n_feats)\n",
    "        self.decoder_high = ARFB(n_feats)\n",
    "        self.conv = Conv(2*n_feats, n_feats, 1, 1, 0, relu=True)\n",
    "        self.attention = CA(n_feats)\n",
    "        self.alise = ARFB(n_feats)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.avgpool(x1)\n",
    "        high = x1 - F.interpolate(x2, size = x.size()[-2:], mode='bilinear', align_corners=True)    # HFM\n",
    "\n",
    "        high = self.decoder_high(high)\n",
    "\n",
    "        for i in range(5):\n",
    "            x2 = self.decoder_low(x2)\n",
    "        x2 = F.interpolate(x2, size = x.size()[-2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return self.alise(self.attention(self.conv(torch.cat([x2, high], dim=1)))) + x\n",
    "    \n",
    "class LCB(nn.Module):\n",
    "    def __init__(self, n_feats, n_blocks):\n",
    "        super(LCB, self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.encoders = nn.ModuleList()\n",
    "        for _ in range(n_blocks):\n",
    "            self.encoders.append(HPB(n_feats))\n",
    "\n",
    "        self.encoders = nn.Sequential(*self.encoders)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        blocks_res = []\n",
    "        for i in range(self.n_blocks):\n",
    "            x = self.encoders[i](x)\n",
    "            blocks_res.append(x)\n",
    "        return blocks_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight transformer backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_patches(images, out_size, ksizes, strides, padding):\n",
    "    unfold = torch.nn.Fold(output_size = out_size, \n",
    "                            kernel_size=ksizes, \n",
    "                            dilation=1, \n",
    "                            padding=padding, \n",
    "                            stride=strides)\n",
    "    patches = unfold(images)\n",
    "    return patches\n",
    "\n",
    "def same_padding(images, ksizes, strides, rates):\n",
    "    assert len(images.size()) == 4\n",
    "    batch_size, channel, rows, cols = images.size()\n",
    "    out_rows = (rows + strides[0] - 1) // strides[0]\n",
    "    out_cols = (cols + strides[1] - 1) // strides[1]\n",
    "    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n",
    "    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n",
    "    padding_rows = max(0, (out_rows-1)*strides[0]+effective_k_row-rows)\n",
    "    padding_cols = max(0, (out_cols-1)*strides[1]+effective_k_col-cols)\n",
    "    # Pad the input\n",
    "    padding_top = int(padding_rows / 2.)\n",
    "    padding_left = int(padding_cols / 2.)\n",
    "    padding_bottom = padding_rows - padding_top\n",
    "    padding_right = padding_cols - padding_left\n",
    "    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n",
    "    images = torch.nn.ZeroPad2d(paddings)(images)\n",
    "    return images\n",
    "\n",
    "def extract_image_patches(images, ksizes, strides, rates):\n",
    "    # images: [B, C, W, H]\n",
    "    images = same_padding(images, ksizes, strides, rates)\n",
    "\n",
    "    unfold = torch.nn.Unfold(kernel_size=ksizes,\n",
    "                             dilation=rates,\n",
    "                             padding=0,\n",
    "                             stride=strides)\n",
    "    patches = unfold(images)\n",
    "    return patches  # [N, C*k*k, L] L=WH\n",
    "\n",
    "class EMHA(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        self.reduce = nn.Linear(dim, dim//2, bias=qkv_bias)\n",
    "        self.qkv = nn.Linear(dim//2, dim//2 * 3, bias=qkv_bias)\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.expand = nn.Linear(dim//2, dim)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.reduce(x)\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q_all = torch.split(q, math.ceil(N//4), dim=-2)\n",
    "        k_all = torch.split(k, math.ceil(N//4), dim=-2)\n",
    "        v_all = torch.split(v, math.ceil(N//4), dim=-2)\n",
    "\n",
    "        output = []\n",
    "        for q,k,v in zip(q_all, k_all, v_all):\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            trans_x = (attn @ v).transpose(1, 2)\n",
    "\n",
    "            output.append(trans_x)\n",
    "\n",
    "        x = torch.cat(output,dim=1)\n",
    "        x = x.reshape(B,N,C)\n",
    "        x = self.expand(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # pointwise feed forward\n",
    "    # potentially enriches the attention output\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features//4\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class ET(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(ET, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attention = EMHA(dim=dim, num_heads=8)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=dim//4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = extract_image_patches(x, ksizes=[3, 3],\n",
    "                                     strides=[1, 1],\n",
    "                                     rates=[1, 1])\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class LTB(nn.Module):\n",
    "    def __init__(self, n_feats, n_lcb_blocks):\n",
    "        super(LTB, self).__init__()\n",
    "        self.attention = ET(dim=288)\n",
    "        self.reduce = nn.Conv2d(n_lcb_blocks * n_feats, n_feats, 3, padding=1)\n",
    "        self.alise = nn.Conv2d(n_feats, n_feats, 3, padding=1)\n",
    "        self.weight1 = Scale(1)\n",
    "        self.weight2 = Scale(1)\n",
    "\n",
    "    def forward(self, x, lcb_res):\n",
    "        _,_,h,w = lcb_res[-1].shape\n",
    "        out = self.attention(self.reduce(torch.cat(lcb_res,dim=1)))\n",
    "        out = out.permute(0,2,1)\n",
    "        out = reverse_patches(out, (h,w), (3,3), 1, 1)\n",
    "        out = self.alise(out)\n",
    "        return self.weight1(x) + self.weight2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESRT(nn.Module):\n",
    "    def __init__(self, upscale=3):\n",
    "        super(ESRT, self).__init__()\n",
    "        n_feats = 32\n",
    "        kernel_size = 3\n",
    "        lcb_blocks = 3\n",
    "\n",
    "        self.head = nn.Conv2d(3, n_feats, kernel_size, padding=1)\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            LCB(n_feats, lcb_blocks),\n",
    "            LTB(n_feats, 3)\n",
    "        )\n",
    "\n",
    "        self.tail1 = nn.Sequential(\n",
    "                Upsampler(upscale, n_feats),\n",
    "                nn.Conv2d(n_feats, 3, kernel_size, padding=1)\n",
    "        )\n",
    "        self.tail2 = nn.Sequential(\n",
    "                Upsampler(upscale, n_feats),\n",
    "                Conv(n_feats, 3, kernel_size, 1, 1, relu=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res1 = self.head(x)\n",
    "        res2 = res1\n",
    "\n",
    "        lcb_blocks_res = self.body[0](res1)\n",
    "        res1 = self.body[1](res1, lcb_blocks_res)\n",
    "\n",
    "        res1 = self.tail1(res1)\n",
    "        res2 = self.tail2(res2)\n",
    "        \n",
    "        return res1 + res2\n",
    "\n",
    "    def chunks_forward(self, x, scale, overlap=10, min_size=60000):\n",
    "        b,c,w,h = x.size()\n",
    "        w,h = w//2, h//2\n",
    "        lr_chunks = [\n",
    "            x[:, :, :w+overlap, :h+overlap],\n",
    "            x[:, :, w-overlap:, :h+overlap],\n",
    "            x[:, :, :w+overlap, h-overlap:],\n",
    "            x[:, :, w-overlap:, h-overlap:]\n",
    "        ]\n",
    "\n",
    "        b,c,w,h = lr_chunks[0].size()\n",
    "        if w*h < min_size:\n",
    "            hr_chunks = []\n",
    "            for lr_chunk in lr_chunks:\n",
    "                hr_chunks.append(self(lr_chunk))\n",
    "        else:\n",
    "            hr_chunks = [\n",
    "                self.chunks_forward(lr_chunk, scale, min_size=min_size) for lr_chunk in lr_chunks\n",
    "            ]\n",
    "\n",
    "        b,c,w,h = hr_chunks[0].size()\n",
    "        overlap *= scale\n",
    "        c1 = hr_chunks[0][:, :, :w-overlap, :h-overlap]\n",
    "        c2 = hr_chunks[1][:, :, overlap:, :h-overlap]\n",
    "        c3 = hr_chunks[2][:, :, :w-overlap, overlap:]\n",
    "        c4 = hr_chunks[3][:, :, overlap:,  overlap:]\n",
    "\n",
    "        return torch.cat((torch.cat((c1, c2), dim=2), torch.cat((c3, c4), dim=2)), dim=3)\n",
    "\n",
    "    def process_image(self, lr_tensor, upscale_factor, chop_size=60000):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            hr_tensor = self.chunks_forward(lr_tensor.unsqueeze(0).to(device), upscale_factor, min_size=chop_size).squeeze().cpu()\n",
    "        return hr_tensor\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"ESRT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        eval_dataloader, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        learning_rate, \n",
    "        upscale_factor,\n",
    "        trained_path, \n",
    "        start_epoch=1, \n",
    "        end_epoch=50,\n",
    "        device='cpu'):\n",
    "    \n",
    "    tqdmEpoch = start_epoch\n",
    "\n",
    "    num_epochs = end_epoch-start_epoch+1\n",
    "    \n",
    "    best_psnr = -float('inf')\n",
    "    best_ssim = -float('inf')\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = -1\n",
    "\n",
    "    with tqdm(total=len(train_dataloader) * num_epochs, desc=f'Epoch {tqdmEpoch}/{end_epoch}', unit='patches') as pbar:\n",
    "        for epoch in range(start_epoch, end_epoch + 1):\n",
    "            # eval\n",
    "            avg_psnr, avg_ssim = eval(model, eval_dataloader, device, upscale_factor)\n",
    "            if avg_psnr > best_psnr and avg_ssim > best_ssim:\n",
    "                best_psnr = avg_psnr\n",
    "                best_ssim = avg_ssim\n",
    "                best_epoch = tqdmEpoch-1\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            pbar.set_description_str(f'Epoch {tqdmEpoch}/{end_epoch} | PSNR: {avg_psnr} | SSIM: {avg_ssim}', refresh=True)\n",
    "\n",
    "            # decrease lr in half every 200 epochs\n",
    "            cur_lr = optimizer.param_groups[0]['lr']\n",
    "            factor = epoch // 200\n",
    "            lr = learning_rate * (0.5 ** factor)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            if cur_lr != lr:\n",
    "                print(f\"New learning rate {lr} at epoch {epoch}\")\n",
    "\n",
    "            # train\n",
    "            avg_loss = train(model, train_dataloader, optimizer, criterion, pbar, device)\n",
    "\n",
    "            tqdmEpoch += 1\n",
    "            pbar.set_postfix(loss=avg_loss)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                save(model.state_dict(), model.get_name(), epoch, trained_path)\n",
    "\n",
    "        # final eval\n",
    "        avg_psnr, avg_ssim = eval(model, eval_dataloader, device, upscale_factor)\n",
    "        if avg_psnr > best_psnr and avg_ssim > best_ssim:\n",
    "            best_epoch = tqdmEpoch-1\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        pbar.set_description_str(f'Epoch {tqdmEpoch-1}/{end_epoch} | PSNR: {avg_psnr} | SSIM: {avg_ssim}', refresh=True)\n",
    "\n",
    "    save(best_weights, model.get_name(), best_epoch, trained_path, best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-4\n",
    "start_epoch=101\n",
    "end_epoch=150\n",
    "upscale_factor = 3\n",
    "patch_size = 96 # | 96 for x2 | 144 for x3 | 192 for x4 |\n",
    "num_workers = 7\n",
    "\n",
    "train_path = \"./Datasets/DIV2K\"\n",
    "eval_path = \"./Datasets/Set5\"\n",
    "\n",
    "set5_path = \"./Datasets/Set5\"\n",
    "set14_path = \"./Datasets/Set14\"\n",
    "urban100_path = \"./Datasets/urban100\"\n",
    "bsd100_path = \"./Datasets/BSD100\"\n",
    "manga109_path = \"./Datasets/manga109\"\n",
    "\n",
    "\n",
    "mode = \"load\"\n",
    "# mode =  \"train\"\n",
    "# mode = \"load-train\"\n",
    "\n",
    "trained_path = \"TrainedModels/esrt/X\" + str(upscale_factor) + \"/\"\n",
    "\n",
    "# load_model_name = \"esrt_best_130.pt\"  # x2\n",
    "load_model_name = \"esrt_best_450.pt\"  # x3\n",
    "# load_model_name = \"esrt_best_340.pt\"  # x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Div2kDataset(train_path, train=True, repeat=10, upscale_factor=upscale_factor, patch_size=patch_size)\n",
    "eval_dataset = ImageDataset(eval_path, upscale_factor=upscale_factor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=num_workers, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESRT(upscale_factor).to(device)\n",
    "criterion = nn.L1Loss().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if mode == \"train\":\n",
    "    train_model(model, train_dataloader, eval_dataloader, criterion, optimizer, learning_rate, upscale_factor, trained_path, start_epoch, end_epoch, device)\n",
    "elif mode == \"load-train\":\n",
    "    model.load_state_dict(torch.load(trained_path + load_model_name, weights_only=False))\n",
    "    print(\"Loaded model: \" + load_model_name)\n",
    "    train_model(model, train_dataloader, eval_dataloader, criterion, optimizer, learning_rate, upscale_factor, trained_path, start_epoch, end_epoch, device)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(trained_path + load_model_name, weights_only=False))\n",
    "    print(\"Loaded model: \" + load_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set5_dataset = ImageDataset(set5_path, upscale_factor=upscale_factor)\n",
    "set14_dataset = ImageDataset(set14_path, upscale_factor=upscale_factor)\n",
    "# bsd100_dataset = ImageDataset(bsd100_path, upscale_factor=upscale_factor)\n",
    "# urban100_dataset = ImageDataset(urban100_path, upscale_factor=upscale_factor)\n",
    "# manga109_dataset = ImageDataset(manga109_path, upscale_factor=upscale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_comparison_picture(model, set5_dataset, 2, (50, 50), scale=upscale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model:\", load_model_name)\n",
    "bic_psnr, bic_ssim, up_psnr, up_ssim = get_avg_metrics(model, set5_dataset, upscale_factor)\n",
    "print(\"Set5\\n\", f\"Bicubic / {round(bic_psnr, 4)} / {round(bic_ssim, 4)}\\n\", f\"ESRT / {round(up_psnr, 4)} / {round(up_ssim, 4)}\")\n",
    "\n",
    "bic_psnr, bic_ssim, up_psnr, up_ssim = get_avg_metrics(model, set14_dataset, upscale_factor)\n",
    "print(\"Set14\\n\", f\"Bicubic / {round(bic_psnr, 4)} / {round(bic_ssim, 4)}\\n\", f\"ESRT / {round(up_psnr, 4)} / {round(up_ssim, 4)}\")\n",
    "\n",
    "# bic_psnr, bic_ssim, up_psnr, up_ssim = get_avg_metrics(model, bsd100_dataset, upscale_factor)\n",
    "# print(\"BSD100\\n\", f\"Bicubic / {round(bic_psnr, 4)} / {round(bic_ssim, 4)}\\n\", f\"ESRT / {round(up_psnr, 4)} / {round(up_ssim, 4)}\")\n",
    "\n",
    "# bic_psnr, bic_ssim, up_psnr, up_ssim = get_avg_metrics(model, urban100_dataset, upscale_factor, chop_size=20000)\n",
    "# print(\"urban100\\n\", f\"Bicubic / {round(bic_psnr, 4)} / {round(bic_ssim, 4)}\\n\", f\"ESRT / {round(up_psnr, 4)} / {round(up_ssim, 4)}\")\n",
    "\n",
    "# bic_psnr, bic_ssim, up_psnr, up_ssim = get_avg_metrics(model, manga109_dataset, upscale_factor, chop_size=25000)\n",
    "# print(\"manga109\\n\", f\"Bicubic / {round(bic_psnr, 4)} / {round(bic_ssim, 4)}\\n\", f\"ESRT / {round(up_psnr, 4)} / {round(up_ssim, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscale_factor = 3\n",
    "\n",
    "dataset_name = \"Set5\"\n",
    "# dataset_name = \"Set14\"\n",
    "# dataset_name = \"BSD100\"\n",
    "# dataset_name = \"urban100\"\n",
    "# dataset_name = \"manga109\"\n",
    "\n",
    "gt_path = f\"./Datasets/{dataset_name}/{dataset_name}_HR\"\n",
    "up_path_srcnn = f\"./Results/{dataset_name}/SRCNN/X{str(upscale_factor)}\"\n",
    "up_path_esrt = f\"./Results/{dataset_name}/ESRT/X{str(upscale_factor)}\"\n",
    "up_path_esrt_paper = f\"./Results/{dataset_name}/ESRT_paper/X{str(upscale_factor)}\"\n",
    "\n",
    "print(f\"{dataset_name} x{upscale_factor} results for:\")\n",
    "print(\"SRCNN\")\n",
    "metrics_from_results(gt_path, up_path_srcnn)\n",
    "print(\"ESRT\")\n",
    "metrics_from_results(gt_path, up_path_esrt)\n",
    "print(\"ESRT paper\")\n",
    "metrics_from_results(gt_path, up_path_esrt_paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
